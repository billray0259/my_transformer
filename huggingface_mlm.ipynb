{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset eli5 (/home/bill/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '1i3n2x',\n",
       " 'title': 'A question about treeline elivations in the Western US.',\n",
       " 'selftext': 'I was part of a discussion in /r/hiking about the treeline on Mt Hood in Oregon and why it is so low as compared to the treeline in the Rocky Mountains at the same latitude (5k feet vs 10k feet respectively). One redditor suggested that it was due to costal winds but I pointed out that the treeline in Southern California, which experiences the same wind patterns as costal Oregon, is about 10k feet, which I understand would be even higher if not for dry conditions. \\n\\nI do understand that treelines become lower as you go north with treelines as low as 1k feet in Alaska but what makes for such a disproportionally low treeline on Mt Hood? ',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['cb0pi2s'],\n",
       "  'text': ['Treeline is affected by a wide variety of factors, any number of which could influence this.  The trees will grow until they can\\'t, which is determined by how \"hard\" it is for them.  The main ones we focus on are:\\n-Temperature: generally colder is worse, its colder higher up.  Frozen soil is bad.\\n-Humidity: its often drier higher up, making it harder for growth.\\n-Aspect:  North facing slopes have less sun, but south facing slopes also are drier.  East/West/North/South goodness also are affected by weather patterns.\\n-Soil: both composition and depth matter.  Highly acidic soils make it harder for most trees to grow, same thing for shallower soils or ones permeated with groundwater.\\n-Local Weather Patterns:  obviously worse weather, like high winds, makes it harder for trees to grow.  M oist winds can cause condensation, which can make it easier.\\n-Tree Species:  different tree species have different toughnesses.  A primarily piney ecosystem will often have a higher treeline than a deciduous one.\\n\\nThese all play with each other too, so it is a pretty complex combo of factors.  Basically, trees grow until they can\\'t anymore.  I don\\'t know exactly what causes the Mt Hood vs. Rockies difference.'],\n",
       "  'score': [2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '1i3n2x',\n",
       " 'title': 'A question about treeline elivations in the Western US.',\n",
       " 'selftext': 'I was part of a discussion in /r/hiking about the treeline on Mt Hood in Oregon and why it is so low as compared to the treeline in the Rocky Mountains at the same latitude (5k feet vs 10k feet respectively). One redditor suggested that it was due to costal winds but I pointed out that the treeline in Southern California, which experiences the same wind patterns as costal Oregon, is about 10k feet, which I understand would be even higher if not for dry conditions. \\n\\nI do understand that treelines become lower as you go north with treelines as low as 1k feet in Alaska but what makes for such a disproportionally low treeline on Mt Hood? ',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['cb0pi2s'],\n",
       " 'answers.text': ['Treeline is affected by a wide variety of factors, any number of which could influence this.  The trees will grow until they can\\'t, which is determined by how \"hard\" it is for them.  The main ones we focus on are:\\n-Temperature: generally colder is worse, its colder higher up.  Frozen soil is bad.\\n-Humidity: its often drier higher up, making it harder for growth.\\n-Aspect:  North facing slopes have less sun, but south facing slopes also are drier.  East/West/North/South goodness also are affected by weather patterns.\\n-Soil: both composition and depth matter.  Highly acidic soils make it harder for most trees to grow, same thing for shallower soils or ones permeated with groundwater.\\n-Local Weather Patterns:  obviously worse weather, like high winds, makes it harder for trees to grow.  M oist winds can cause condensation, which can make it easier.\\n-Tree Species:  different tree species have different toughnesses.  A primarily piney ecosystem will often have a higher treeline than a deciduous one.\\n\\nThese all play with each other too, so it is a pretty complex combo of factors.  Basically, trees grow until they can\\'t anymore.  I don\\'t know exactly what causes the Mt Hood vs. Rockies difference.'],\n",
       " 'answers.score': [2],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dadb7d2c0bd43e087a5214d58d950bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9b6ba1266c45c685bb4330c9cddd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c751038cc37344e0959f6446ca593fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca283c48c1d4f8bb55bcb97561f175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bc9fc5a4eb44b1970e59a88e229d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3154fb997f4332ac2aca3a4945c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e99b26b8ca4053a9a5e324ffe48bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b44960feb95420eaa475834a65727c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86d297769614b88a98835bd4baae133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6347b4a176549b6964656a467c64de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b83334814d34592b2c3f968971f4b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e85ebafb7494aa4b53ea526421dd813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa1073c3aa34aac94bce26e1adb90e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04ad43801da40329281b23bee329d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0c4cc5369c40589a6f5999cf6616aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cb7a8ac15c4313bc21b252464325e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7463\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2799\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  45/2799 00:03 < 04:14, 10.82 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:707\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=705'>706</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=706'>707</a>\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=708'>709</a>\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=709'>710</a>\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=710'>711</a>\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=711'>712</a>\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=712'>713</a>\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=713'>714</a>\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 128 at dim 1 (got 76)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bill/dev/my_transformer/huggingface_mlm.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=13'>14</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=14'>15</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bobiwan/home/bill/dev/my_transformer/huggingface_mlm.ipynb#ch0000010vscode-remote?line=16'>17</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1403'>1404</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1405'>1406</a>\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1406'>1407</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1407'>1408</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1408'>1409</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1409'>1410</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1410'>1411</a>\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1411'>1412</a>\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1412'>1413</a>\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1413'>1414</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py:1625\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1621'>1622</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1623'>1624</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1624'>1625</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1625'>1626</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1626'>1627</a>\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1627'>1628</a>\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/trainer.py?line=1628'>1629</a>\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py:42\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=40'>41</a>\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=42'>43</a>\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py:729\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=725'>726</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, examples: List[Union[List[\u001b[39mint\u001b[39m], Any, Dict[\u001b[39mstr\u001b[39m, Any]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=726'>727</a>\u001b[0m     \u001b[39m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=727'>728</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples[\u001b[39m0\u001b[39m], Mapping):\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=728'>729</a>\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(examples, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of)\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=729'>730</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=730'>731</a>\u001b[0m         batch \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=731'>732</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: _torch_collate_batch(examples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/data/data_collator.py?line=732'>733</a>\u001b[0m         }\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2894\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2890'>2891</a>\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m   <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2891'>2892</a>\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m-> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=2893'>2894</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:209\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=204'>205</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=206'>207</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=208'>209</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:723\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=717'>718</a>\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=718'>719</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=719'>720</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=720'>721</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=721'>722</a>\u001b[0m             )\n\u001b[0;32m--> <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=722'>723</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=723'>724</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=724'>725</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=725'>726</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/bill/dev/my_transformer/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py?line=727'>728</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b79a7f5a46152e71cff8647b5c3b8d14e4a3aaa5c5e0a4b675db2944c297bd15"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
