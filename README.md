# my_transformer


I'm implementing the papers [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) and [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) to apply what I have learned about transformer architectures.
